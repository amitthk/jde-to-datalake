#+TITLE: Using Apache Airflow to integrate JD Edwards ERP to Enterprise Data Lake and External System
#+AUTHOR: amitthk
#+DATE: 2025-08-21
#+OPTIONS: toc:2 num:t

* Overview

How do we extract critical business insights from ERP systems without compromising their performance or integrity? This repository we demonstrate through an example how we safely extracted data from JD Edwards ERP into in-house data lake using Apache Airflow. The system implements data validation and compliance checks to ensure data integrity while maintaining minimal impact on ERP performance through non-intrusive extraction methods. The extracted data feeds an Operations Management system for a bakery, enabling real-time time series analytics and business insights while preserving full audit trails for compliance and historical analyses.

Systems like JD Edwards are the backbone of business operations, storing the most accurate financial and operational data at any given moment. However, when you need to unlock deeper insights through time series analysis, diagnostics, and predictive analytics, directly querying the ERP can slow down critical business processes. The sensible approach will be to create a seamless data pipeline that feeds this valuable information into a data lake, where we will then perform complex analytics and support additional business applications without ever touching your production ERP systemâ€™s performance. This repository presents a demonstration system - a simplified version of a enterprise data integration platform.

*Note:* All organization-specific information, proprietary business logic, and sensitive configurations have been removed or anonymized. This example system cannot be deployed as-is in a production environment; a real-world implementation would require comprehensive security frameworks, compliance controls, audit mechanisms, performance optimization, and integration with existing enterprise infrastructure.

The system showcases a data pipeline architecture that integrates JD Edwards ERP and operations management systems, specifically demonstrated through a bakery operations use case here. This implementation leverages Apache Airflow as the orchestration engine to create resilient, monitored data pipelines that ensure data integrity while providing real-time visibility into inventory movements and operational metrics.

** Architecture Overview

The architecture (shown in ~images/jde_to_datalake.svg~) shows a multi-tier system for data integration:

[[file:images/jde_to_datalake.svg]]

*** Core Components Explanation

**** JD Edwards ERP Integration Layer
The system connects with JD Edwards through API endpoints to extract inventory data from the Cardex system. This integration handles the task of retrieving inventory movements, stock levels, and product master data while maintaining the integrity of the source ERP system. The JDE helper module (~backend/jde_helper.py~) manages authentication, data extraction, and handles errors properly.

**** Apache Airflow Orchestration Engine
Apache Airflow handles data pipeline orchestration, providing:
- *Scheduled Data Synchronization*: Automated workflows that run at set intervals to keep data fresh
- *Error Handling & Retry Logic*: Retry mechanisms with exponential backoff for handling temporary failures
- *Data Lineage Tracking*: Full audit trail of data transformations and movements
- *Monitoring & Alerting*: Real-time view of pipeline health and performance

**** S3 compatible Data Lake Infrastructure
The S3 data lake stores all data assets, organized in a medallion architecture pattern:
- *Bronze Layer*: Raw data with schema-on-read capabilities
- *Silver Layer*: Cleaned and standardized data with data quality rules
- *Gold Layer*: Business-ready datasets optimized for analytics and reporting
- *Audit Trail Storage*: Historical record of all data operations and transformations

**** PostgreSQL Metadata & Control Database
PostgreSQL is used as backend for Aiflow and Compliance dashboard. Besides this it manages the operational metadata including:
- Schema evolution tracking with automatic DDL generation
- Data pipeline execution logs and performance metrics
- User session management and authentication tokens
- Configuration management for dynamic pipeline behavior

**** React.js Compliance Dashboard
The frontend application provides operational oversight:
- Real-time monitoring of data pipeline execution status
- Interactive data validation for outlier detection and correction
- Compliance reporting with drill-down into data lineage
- Manual override capabilities for exception handling

** Airflow Data Pipeline Implementation

The system implements two primary data flow patterns through Airflow DAGs:

*** JDE Cardex to External System Pipeline (~dag_cardex_changes_to_bakery_system.py~)

This pipeline handles the extraction of inventory data from JDE Cardex and transforms it for use by external operational systems:

#+BEGIN_SRC python
def get_jde_cardex_with_comparison(bu: str, days_back: int = 5) -> dict:
    """
    Extracts and compares JDE cardex data
    with external system inventory records.
    """
    load_dotenv()
    
    # Calculate extraction window
    today = datetime.now()
    start_date = today - timedelta(days=days_back)
    date_str = start_date.strftime('%d/%m/%Y')
    
    print(f"Fetching JDE cardex data for BU {bu} since {date_str}")
    
    # Extract from JDE with error handling
    from jde_helper import get_latest_jde_cardex
    jde_data = get_latest_jde_cardex(bu, date_str)
    
    if not jde_data or 'ServiceRequest1' not in jde_data:
        print(f"No JDE data found for BU {bu}")
        return None
    
    # Transform JDE data structure for downstream processing
    jde_transactions = jde_data['ServiceRequest1']['fs_DATABROWSE_V4111A']['data']['gridData']['rowset']
    df_jde = pd.DataFrame(jde_transactions)
    
    # Parallel extraction from external system
    from bakery_helper import get_data_from_bakery_system
    bakery_system_data = get_data_from_bakery_system()
    
    # Execute data quality validation and unit conversions
    return perform_data_reconciliation(df_jde, bakery_system_data)
#+END_SRC

The pipeline includes unit conversion logic that handles the mapping between JDE's unit of measure system and operational requirements:

#+BEGIN_SRC python
def convert_unit_quantity(quantity: float, from_unit: str, to_unit: str) -> float:
    """
    Performs unit conversion with validation and error handling.
    Supports conversions like weight-to-volume for different ingredients.
    """
    try:
        # Apply conversion factors based on ingredient properties
        conversion_result = perform_conversion_with_validation(quantity, from_unit, to_unit)
        
        # Log conversion for audit trail
        log_conversion_event(quantity, from_unit, to_unit, conversion_result)
        
        return conversion_result
    except ConversionError as e:
        # Route to manual validation queue
        queue_for_manual_review(quantity, from_unit, to_unit, str(e))
        raise
#+END_SRC

*** External System to JDE Synchronization Pipeline (~dag_bakery_system_to_jde.py~)

This reverse pipeline handles the task of synchronizing consumption data back to JDE for accurate inventory management:

#+BEGIN_SRC python
def process_inventory_consumption_batch():
    """
    Processes inventory consumption data
    from operational systems back to JDE for accurate inventory tracking.
    """
    # Extract consumption data with data quality validation
    consumption_data = extract_validated_consumption_data()
    
    # Apply business rules and unit conversions
    processed_data = apply_jde_transformation_rules(consumption_data)
    
    # Execute batch processing with error handling
    for batch in chunk_data_for_processing(processed_data):
        try:
            result = submit_ii_transaction_to_jde(batch)
            log_transaction_result(batch, result)
        except JDETransactionError as e:
            handle_transaction_failure(batch, e)
#+END_SRC

** Data Validation & Compliance Framework

The system includes data validation mechanisms that ensure data integrity throughout the pipeline:

*** Validation Dashboard - JDE Cardex to External System

[[file:images/jde_cardex_to_external_system_validation.png]]

This dashboard (~jde_cardex_to_external_system.png~) provides a checkpoint where data analysts can:
- Review inventory movements extracted from JDE Cardex before synchronization
- Identify and investigate outliers or anomalous data patterns
- Validate unit conversions and quantity calculations
- Approve or reject data batches for downstream processing
- Manually correct data discrepancies with full audit trail

*** Validation Dashboard - External System to JDE

[[file:images/external_system_to_jde_validation.png]]

The reverse validation interface (~external_system_to_jde_validation.png~) enables operators to:
- Map consumption data from operational systems to JDE-compatible formats
- Validate ingredient mappings and unit conversions (e.g., sugar consumption in kg to JDE inventory units)
- Review calculated inventory adjustments before committing to JDE
- Dispatch II (Inventory Issues) transactions to JDE with confidence
- Monitor transaction status and handle exceptions

** Production Considerations & Security Framework

*Important Notice*: This demonstration system has been significantly simplified for educational purposes. A production-ready implementation would require:

*** Security & Compliance
- End-to-end encryption for data in transit and at rest
- Role-based access control (RBAC) with multi-factor authentication
- SOX compliance controls for financial data handling
- GDPR/data privacy compliance for customer data
- Audit logging with tamper-evident storage

*** Performance & Scalability
- Horizontal scaling capabilities for high-volume data processing
- Caching strategies for frequently accessed data
- Database partitioning and indexing optimization
- Load balancing and failover mechanisms
- Performance monitoring and alerting

*** Integration & Operations
- Enterprise service bus integration for system-to-system communication
- Comprehensive monitoring with tools like Prometheus and Grafana
- Automated deployment pipelines with CI/CD integration
- Disaster recovery and business continuity planning
- Change management and version control for pipeline configurations

The system shows how modern data engineering practices can be applied to ERP integration challenges, demonstrating how Apache Airflow can serve as an orchestration platform for data pipelines while maintaining the operational visibility and control required for business processes.

* Key Features

** Bi-directional Data Flow
The system enables seamless data exchange between JDE and Bakery Operations:
- *JDE to Bakery Operations*: Transfers inventory updates and product master data
- *Bakery Operations to JDE*: Sends usage and consumption data along with inventory adjustments

** S3 Data Lake Integration
All data operations are captured and stored in a  data lake:
- Data flows are stored as Parquet files in S3 for efficient querying
- Files are organized by date and operation type for easy navigation
- Schema versions are tracked automatically as data structures evolve
- Complete audit trail maintains compliance and supports analytics

** Web-based Dashboard
The intuitive dashboard provides complete system oversight:
- Monitor data flows in real-time across all system components
- Control batch processing operations with detailed progress tracking
- Explore and download data directly from S3 storage
- Manage database schemas and track their evolution over time

** Schema Management
The system automatically handles data structure changes:
- Schemas are inferred automatically from incoming data
- All schema versions are tracked with timestamps in PostgreSQL
- DDL scripts are generated automatically for new table structures
- Schema evolution ensures backward compatibility is maintained

** Internal Bakery Operations Endpoints
The system includes self-contained API endpoints that eliminate external dependencies:
- Built-in endpoints are available under the ~/bakeryops/~ path
- No external bakery operations system is required for basic functionality
- Data is stored in memory with automatic S3 backup for audit purposes
- Mock data generation tools are included for testing and development

** Data Pipeline Orchestration

This system uses Apache Airflow to manage data integration workflows. Unlike simple ETL tools, this implementation uses Airflow's capabilities to create self-healing, monitored data pipelines that handle the details of ERP integration.

*** Airflow Features Implementation
- *Dynamic DAG Generation*: Pipelines adapt to changing data schemas and business requirements
- *Retry Logic*: Exponential backoff with jittering for handling temporary ERP system issues
- *Data Quality Validation*: Built-in data profiling and anomaly detection before data commits
- *Parallel Processing*: Task parallelization for optimal resource utilization
- *Cross-System Transaction Management*: Ensures ACID properties across distributed systems

*** Real-Time Monitoring & Alerting
The system provides operational visibility through:
- Pipeline execution dashboards with drill-down capabilities
- Automated alerting for data quality violations and system failures
- Performance metrics tracking with historical trend analysis
- Custom business rule validation with configurable thresholds

** Data Lake Architecture

The S3-based data lake implements best practices for large-scale data management:

*** Medallion Architecture Pattern
- *Bronze Layer*: Raw data ingestion with schema-on-read flexibility
- *Silver Layer*: Cleansed, validated data with enforced quality rules
- *Gold Layer*: Business-ready datasets optimized for analytics consumption

*** Data Governance
- Automated data cataloging with schema evolution tracking
- Data lineage visualization from source systems to final consumption
- Time-travel capabilities for historical data analysis
- Compliance reporting for regulatory requirements

* System Components

** Backend Services

*** Core Services
The backend infrastructure is built around several integrated services. The FastAPI application serves as the main API server (~backend/main.py~) with built-in internal bakery operations endpoints available at ~/bakeryops/*~. Data lake operations are managed through the S3 helper module (~backend/s3_helper.py~), while schema tracking and versioning is handled by the schema manager (~backend/schema_manager.py~). Integration with JDE systems is provided through the JDE helper (~backend/jde_helper.py~).

*** Helper Modules
Supporting functionality includes the bakery operations helper for API integration (~backend/bakery_ops_helper.py~), JWT-based authentication system (~backend/auth.py~),  user session management (~backend/session_helper.py~), and shared utility functions (~backend/utility.py~).

** Data Pipeline (Airflow DAGs)
The system includes automated data pipeline workflows for data synchronization. The JDE Cardex to Bakery Operations pipeline (~backend/dags/dag_cardex_changes_to_bakery_ops.py~) handles inventory transfers, while the Bakery Operations to JDE synchronization (~backend/dags/dag_bakery_ops_to_jde.py~) manages usage and consumption data flow back to the JDE system.

** Frontend Application
The user interface is built as a React-based dashboard (~ui/src/~) with an organized component structure. The main application component (~App.js~) handles the overall user experience, while the ~components/~ directory contains reusable UI elements for data visualization and user interaction. State management is handled through React context (~context/~), and API configuration is centralized in the ~config/~ directory.

** Database Schema
PostgreSQL serves as the database for metadata tracking and audit trails. The database schema is defined in ~backend/create_bakery_ops_tables.sql~ and includes all necessary tables for tracking system operations, data transformations, and user activities.

* Installation & Setup

** Prerequisites

*** System Requirements
The following software components are required for system operation:

Python version 3.8 or higher is needed for all backend services, while Node.js version 16.x or higher is required for the frontend application. PostgreSQL version 12.x or higher handles metadata and tracking operations. An AWS account is required for S3 data lake functionality, though this is optional during development phases.

*** Development Tools
Several tools are recommended for effective development and system administration:

Version control and collaboration are managed through ~git~, while API testing and debugging can be performed using ~curl~. Database management and queries are handled through ~psql~, and S3 operations require ~aws-cli~ when utilizing cloud storage features.

** Quick Start

*** 1. Repository Setup
Start by cloning the repository and preparing the environment configuration:

#+BEGIN_SRC bash
# Clone the repository
git clone <repository-url>
cd jde-to-datalake

# Copy environment template
cp .env.template backend/.env
#+END_SRC

*** 2. Environment Configuration
Edit the ~backend/.env~ file with your specific settings:

#+BEGIN_SRC bash
# Database Configuration
PG_DATABASE_URL=postgresql://username:password@localhost:5432/bakery_operations_db
DB_NAME=bakery_operations_db

# Backend Configuration
BACKEND_BASE_URL=http://localhost:8000

# Facility Configuration
FACILITY_ID=default_facility

# JDE Configuration (update with your JDE server details)
JDE_BUSINESS_UNIT=1110
JDE_CARDEX_URL=https://your-jde-server/jderest/v3/orchestrator/JDE_CARDEX_SUMMARY
JDE_CARDEX_USERNAME=your_username
JDE_CARDEX_PASSWORD=your_password

# S3 Configuration (optional for development)
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
S3_BUCKET_NAME=bakery-operations-data-lake

# Authentication
SECRET_KEY=your-secret-key-change-this-in-production
#+END_SRC

*** 3. Database Setup
Create and initialize the PostgreSQL database with the required schema:

#+BEGIN_SRC bash
# Create database
createdb bakery_operations_db

# Run schema creation
psql -d bakery_operations_db -f backend/create_bakery_ops_tables.sql
#+END_SRC

*** 4. Backend Setup
Set up the Python environment and start the backend service:

#+BEGIN_SRC bash
# Navigate to backend
cd backend

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start the development server
uvicorn main:app --reload --host 0.0.0.0 --port 8000
#+END_SRC

The backend will be available at: http://localhost:8000

*** 5. Frontend Setup
In a separate terminal, set up and start the React frontend:

#+BEGIN_SRC bash
# Navigate to UI directory (in a new terminal)
cd ui

# Install dependencies
npm install

# Start development server
npm start
#+END_SRC

The frontend will be available at: http://localhost:3000

*** 6. Initial Data Setup
Once both services are running, initialize the system with sample data:

#+BEGIN_SRC bash
# Initialize sample data for testing
curl -X POST http://localhost:8000/dev/initialize-sample-data

# Test internal endpoints
curl http://localhost:8000/dev/test-internal-bakery-ops
#+END_SRC

** Production Deployment

*** Using Systemd Services

**** 1. Copy deployment scripts
#+BEGIN_SRC bash
# Make deployment scripts executable
chmod +x deploy/setup-production-systemd.sh
chmod +x deploy/setup-simple-systemd.sh
#+END_SRC

**** 2. Run production setup
#+BEGIN_SRC bash
# For production with Gunicorn
sudo ./deploy/setup-production-systemd.sh

# Or for simple setup
sudo ./deploy/setup-simple-systemd.sh
#+END_SRC

**** 3. Service Management
#+BEGIN_SRC bash
# Start services
sudo systemctl start stical-data-backend
sudo systemctl start stical-data-frontend

# Enable auto-start
sudo systemctl enable stical-data-backend
sudo systemctl enable stical-data-frontend

# Check status
sudo systemctl status stical-data-backend
sudo systemctl status stical-data-frontend
#+END_SRC

*** Manual Production Setup

**** Backend Production
#+BEGIN_SRC bash
# Install production WSGI server
pip install gunicorn

# Run with Gunicorn
cd backend
gunicorn main:app -w 4 -b 0.0.0.0:8000
#+END_SRC

**** Frontend Production
#+BEGIN_SRC bash
# Build for production
cd ui
npm run build

# Serve static files (using serve or nginx)
npx serve -s build -l 3000
#+END_SRC

** Docker Deployment (Optional)

*** Backend Dockerfile
Create ~backend/Dockerfile~:
#+BEGIN_SRC dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
#+END_SRC

*** Frontend Dockerfile
Create ~ui/Dockerfile~:
#+BEGIN_SRC dockerfile
FROM node:16-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=builder /app/build /usr/share/nginx/html
EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
#+END_SRC

*** Docker Compose
Create ~docker-compose.yml~:
#+BEGIN_SRC yaml
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - PG_DATABASE_URL=postgresql://postgres:password@db:5432/bakery_ops
    depends_on:
      - db

  frontend:
    build: ./ui
    ports:
      - "3000:80"
    depends_on:
      - backend

  db:
    image: postgres:13
    environment:
      - POSTGRES_DB=bakery_ops
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
#+END_SRC

* Configuration

** Environment Variables

*** Core Backend Configuration
#+BEGIN_SRC bash
# Database
PG_DATABASE_URL=postgresql://username:password@localhost:5432/database_name
DB_NAME=bakery_operations_db

# Backend API
BACKEND_BASE_URL=http://localhost:8000

# Facility Management
FACILITY_ID=default_facility
#+END_SRC

*** JDE System Configuration
#+BEGIN_SRC bash
JDE_BUSINESS_UNIT=1110
JDE_CARDEX_URL=https://your-jde-server/jderest/v3/orchestrator/JDE_CARDEX_SUMMARY
JDE_CARDEX_USERNAME=your_username
JDE_CARDEX_PASSWORD=your_password
JDE_ITEM_MASTER_UPDATES_URL=https://your-jde-server/jderest/v3/orchestrator/JDE_ITEM_MASTER
JDE_IA_URL=https://your-jde-server/jderest/v3/orchestrator/JDE_INVENTORY_ADJUSTMENTS
#+END_SRC

*** S3 Data Lake Configuration
#+BEGIN_SRC bash
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=us-east-1
S3_BUCKET_NAME=bakery-operations-data-lake
S3_BASE_PREFIX=jde-ingestion
#+END_SRC

*** Authentication Configuration
#+BEGIN_SRC bash
SECRET_KEY=your-secret-key-change-this-in-production-must-be-long-and-random
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# LDAP Configuration (optional)
LDAP_SERVER=ldap://your-ldap-server:389
LDAP_BASE_DN=dc=company,dc=com
LDAP_USER_DN=cn=users,dc=company,dc=com
#+END_SRC

** Frontend Configuration

*** API Configuration
Edit ~ui/src/config/api.js~:
#+BEGIN_SRC javascript
const API_CONFIG = {
  BASE_URL: process.env.REACT_APP_API_URL || 'http://localhost:8000',
  ENDPOINTS: {
    TOKEN: '/token',
    HEALTH: '/health',
    DATA: '/data',
    BAKERY_OPS: '/bakeryops',
    S3: '/s3'
  },
  TIMEOUT: 30000
};

export default API_CONFIG;
#+END_SRC

*** Environment Variables for Frontend
Create ~ui/.env~:
#+BEGIN_SRC bash
REACT_APP_API_URL=http://localhost:8000
REACT_APP_TITLE=STICAL Data Management System
REACT_APP_VERSION=2.0.0
#+END_SRC

* Internal Bakery Operations API

** Available Endpoints

*** Products Management
- ~GET /bakeryops/facilities/{facility_id}/products~ - List products
- ~POST /bakeryops/facilities/{facility_id}/products~ - Create product

*** Inventory Management  
- ~POST /bakeryops/facilities/{facility_id}/inventory-adjustments~ - Create adjustment
- ~GET /bakeryops/facilities/{facility_id}/inventory-movements~ - List movements

*** Development Helpers
- ~POST /bakeryops/facilities/{facility_id}/batch-data~ - Add sample data
- ~POST /dev/initialize-sample-data~ - Initialize test data
- ~GET /dev/test-internal-bakery-ops~ - Test all endpoints

** Data Structure

*** Product Object
#+BEGIN_SRC json
{
  "_id": "prod_001",
  "facility_id": "default_facility", 
  "productName": "Flour",
  "description": "All-purpose flour",
  "productCategory": "Ingredient",
  "inventoryUnit": "KG",
  "onHand": {
    "amount": 100,
    "batches": []
  },
  "archived": false,
  "created_at": "2025-08-21T10:00:00Z",
  "updated_at": "2025-08-21T10:00:00Z"
}
#+END_SRC

*** Movement Object
#+BEGIN_SRC json
{
  "_id": "mov_001",
  "facility_id": "default_facility",
  "productId": "prod_001", 
  "batchNumber": "FLOUR_001",
  "quantity": 10,
  "unit": "KG",
  "adjustmentType": "USAGE",
  "reason": "Production batch 001",
  "adjustmentDate": "2025-08-21T10:00:00Z",
  "vesselCode": "V001",
  "lotNumber": "LOT001"
}
#+END_SRC

* API Endpoints

** Core Data Endpoints
- ~GET /data/df_bakery_ops_expanded~ - Bakery operations products
- ~GET /data/joined_df3~ - JDE vs Bakery Ops comparison
- ~GET /data/jde_item_master_review~ - Item master comparison
- ~GET /data/internal_bakery_ops_expanded~ - Internal bakery ops data

** S3 Data Lake Endpoints
- ~GET /s3/dispatches~ - List S3 stored dispatches
- ~GET /s3/schemas~ - Get schema versions  
- ~GET /s3/download/{s3_key}~ - Download dispatch file

** Dispatch Control Endpoints
- ~GET /data/bakery_ops_to_jde_actions~ - Get pending actions
- ~POST /bakery_ops_to_jde/dispatch~ - Dispatch to JDE
- ~POST /bakery_ops_to_jde/prepare_payload~ - Preview JDE payload

** Authentication Endpoints
- ~POST /token~ - Get authentication token
- ~GET /health~ - Health check (no auth required)

** Development & Testing Endpoints
- ~POST /dev/initialize-sample-data~ - Initialize sample data
- ~GET /dev/test-internal-bakery-ops~ - Test internal endpoints

* Data Flow Patterns

** 1. JDE Cardex Changes â†’ Bakery Operations
#+BEGIN_SRC python
# Fetch JDE cardex data
jde_data = get_latest_jde_cardex(business_unit, date_range)

# Transform and enrich
processed_data = transform_jde_to_bakery_ops_format(jde_data)

# Dispatch to internal Bakery Operations
results = dispatch_to_bakery_operations(processed_data)

# Store in S3 data lake
s3_helper.store_jde_dispatch(processed_data, 'cardex_changes')
#+END_SRC

** 2. Bakery Operations Usage â†’ JDE
#+BEGIN_SRC python
# Fetch usage data from internal Bakery Operations
usage_data = fetch_action_data_from_bakery_operations(start_date)

# Transform to JDE format
jde_payload = transform_to_jde_format(usage_data)

# Dispatch to JDE
jde_response = post_data_to_jde(jde_payload)

# Store results in S3
s3_helper.store_jde_dispatch(jde_response, 'jde_dispatches')
#+END_SRC

** 3. Internal Product Creation
#+BEGIN_SRC python
# Create product via internal API
product_data = {
    'productName': 'New Ingredient',
    'description': 'Description',
    'inventoryUnit': 'KG',
    'productCategory': 'Ingredient'
}

response = requests.post(
    f"{backend_url}/bakeryops/facilities/{facility_id}/products",
    json=product_data
)
#+END_SRC

* S3 Data Lake Structure

#+BEGIN_EXAMPLE
s3://bakery-operations-data-lake/
â”œâ”€â”€ jde-ingestion/
â”‚   â”œâ”€â”€ to_bakery_ops/
â”‚   â”‚   â””â”€â”€ year=2025/month=08/day=21/
â”‚   â”‚       â””â”€â”€ dispatch_20250821_143022.parquet
â”‚   â”œâ”€â”€ from_bakery_ops/  
â”‚   â”‚   â””â”€â”€ year=2025/month=08/day=21/
â”‚   â”‚       â””â”€â”€ dispatch_20250821_143045.parquet
â”‚   â”œâ”€â”€ cardex_changes/
â”‚   â”‚   â””â”€â”€ year=2025/month=08/day=21/
â”‚   â”‚       â””â”€â”€ dispatch_20250821_143100.parquet
â”‚   â”œâ”€â”€ bakery_ops_products/
â”‚   â”‚   â””â”€â”€ year=2025/month=08/day=21/
â”‚   â”‚       â””â”€â”€ products_20250821_143000.parquet
â”‚   â”œâ”€â”€ bakery_ops_movements/
â”‚   â”‚   â””â”€â”€ year=2025/month=08/day=21/
â”‚   â”‚       â””â”€â”€ movements_20250821_143000.parquet
â”‚   â””â”€â”€ schemas/
â”‚       â””â”€â”€ bakery_ops_products/
â”‚           â””â”€â”€ schema_20250821_143000.json
#+END_EXAMPLE

* UI Components

** Main Components

*** App.js
- Main application component
- Handles routing and global state
- Manages authentication context

*** Component Structure
#+BEGIN_EXAMPLE
ui/src/components/
â”œâ”€â”€ AdvancedPatchForm.js      # Advanced ingredient patching
â”œâ”€â”€ BackendStatus.js          # Backend health monitoring  
â”œâ”€â”€ BakeryOpsData.js          # Bakery operations data display
â”œâ”€â”€ BakeryOpsToJde.js         # Dispatch to JDE interface
â”œâ”€â”€ BakerySystemData.js       # Legacy system data (deprecated)
â”œâ”€â”€ BakerySystemToJde.js      # Legacy dispatch interface
â”œâ”€â”€ BarChart.js               # Data visualization
â”œâ”€â”€ BatchReview.js            # Batch processing interface
â”œâ”€â”€ CompareData.js            # Data comparison views
â”œâ”€â”€ ErrorModal.js             # Error handling modal
â”œâ”€â”€ JdeItemMasterReview.js    # JDE item master interface
â”œâ”€â”€ JoinedJDEData.js          # Combined JDE data views
â”œâ”€â”€ LiveDataComparison.js     # Real-time data comparison
â”œâ”€â”€ Login.js                  # Authentication component
â”œâ”€â”€ PivotTable.js             # Data pivot interface
â””â”€â”€ S3DataManager.js          # S3 data lake management
#+END_EXAMPLE

*** Context Management
#+BEGIN_EXAMPLE
ui/src/context/
â””â”€â”€ AuthContext.js            # Authentication state management
#+END_EXAMPLE

*** Configuration
#+BEGIN_EXAMPLE  
ui/src/config/
â””â”€â”€ api.js                    # API endpoint configuration
#+END_EXAMPLE

** Key Features

*** Authentication
- JWT token-based authentication
- Automatic token refresh
- Protected route handling
- Login/logout functionality

*** Data Visualization
- Real-time charts and graphs
- Interactive data tables
- Comparison views
- Export capabilities

*** Batch Processing
- Batch review interface
- Bulk operations
- Progress tracking
- Error handling

* Schema Management

** Automatic Schema Inference
#+BEGIN_SRC python
# Infer schema from data
schema_def = schema_manager.infer_schema_from_data(sample_data)

# Register new schema version
version = schema_manager.register_schema('table_name', schema_def)

# Get current schema  
current = schema_manager.get_current_schema('table_name')
#+END_SRC

** Schema Evolution
- Automatic detection of schema changes
- Version tracking with timestamps
- DDL generation for new tables
- Schema compatibility validation
- Backward compatibility maintenance

** Database Schema Tables
- ~schema_versions~ - Track schema evolution
- ~bakery_ops_products~ - Product information
- ~bakery_ops_movements~ - Inventory movements
- ~dispatch_logs~ - Operation audit trail
- ~session_data~ - User session management

* Monitoring & Maintenance

** Health Checks
- ~GET /health~ - API health status
- Database connection monitoring
- S3 connectivity verification
- JDE system availability
- Internal service status

** Logging & Audit
- All data flows logged to S3
- Database audit trails
- API access logging
- Error tracking and alerting
- Performance metrics collection

** Performance Monitoring
- Data processing metrics
- API response times
- S3 storage utilization
- Database performance
- Memory usage tracking
- Request rate monitoring

** Maintenance Scripts
#+BEGIN_SRC bash
# Check system health
curl http://localhost:8000/health

# View logs
tail -f /var/log/stical-data-backend.log

# Database maintenance
psql -d bakery_operations_db -c "VACUUM ANALYZE;"

# Clear old session data
curl -X DELETE http://localhost:8000/admin/cleanup-sessions
#+END_SRC

* Troubleshooting

** Common Issues

*** Backend Issues

**** Service Won't Start
#+BEGIN_SRC bash
# Check service status
sudo systemctl status stical-data-backend

# View logs
journalctl -u stical-data-backend -f

# Check configuration
cd backend && python -c "from dotenv import load_dotenv; load_dotenv(); import os; print('DB:', os.getenv('PG_DATABASE_URL'))"
#+END_SRC

**** Database Connection Problems
#+BEGIN_SRC bash
# Test database connection
psql $PG_DATABASE_URL -c "SELECT version();"

# Check database exists
psql $PG_DATABASE_URL -c "\l"

# Verify schema
psql $PG_DATABASE_URL -c "\dt"
#+END_SRC

**** S3 Connection Problems
#+BEGIN_SRC bash
# Check AWS credentials
aws s3 ls s3://your-bucket-name/

# Verify IAM permissions
aws iam list-attached-role-policies --role-name your-role

# Test S3 connectivity
curl http://localhost:8000/s3/dispatches
#+END_SRC

*** Frontend Issues

**** Build Failures
#+BEGIN_SRC bash
# Clear npm cache
npm cache clean --force

# Delete node_modules and reinstall
rm -rf node_modules package-lock.json
npm install

# Check for missing dependencies
npm ls
#+END_SRC

**** API Connection Issues
#+BEGIN_SRC bash
# Test backend connectivity
curl http://localhost:8000/health

# Check CORS settings
curl -H "Origin: http://localhost:3000" \
     -H "Access-Control-Request-Method: GET" \
     -H "Access-Control-Request-Headers: X-Requested-With" \
     -X OPTIONS http://localhost:8000/health
#+END_SRC

*** JDE Integration Problems
#+BEGIN_SRC bash
# Check JDE endpoint availability
curl -u $JDE_CARDEX_USERNAME:$JDE_CARDEX_PASSWORD \
     $JDE_CARDEX_URL

# Test JDE authentication
curl -i -u $JDE_CARDEX_USERNAME:$JDE_CARDEX_PASSWORD \
     $JDE_CARDEX_URL

# Verify JDE data format
curl http://localhost:8000/data/joined_df3
#+END_SRC

** Log Files
- *Backend logs*: ~/var/log/stical-data-backend.log~
- *Frontend logs*: Browser console and ~/var/log/stical-data-frontend.log~
- *System logs*: ~journalctl -u stical-data-backend~
- *Database logs*: PostgreSQL logs (location varies by installation)
- *S3 operations*: CloudTrail logs for S3 access

** Performance Troubleshooting

*** Slow API Responses
#+BEGIN_SRC bash
# Check database query performance
psql $PG_DATABASE_URL -c "EXPLAIN ANALYZE SELECT * FROM bakery_ops_products LIMIT 10;"

# Monitor active connections
psql $PG_DATABASE_URL -c "SELECT * FROM pg_stat_activity WHERE state = 'active';"

# Check memory usage
free -h
ps aux | grep python
#+END_SRC

*** High Memory Usage
#+BEGIN_SRC bash
# Monitor backend memory
ps aux | grep uvicorn

# Check database memory
ps aux | grep postgres

# System memory overview
htop
#+END_SRC

* Development

** Running Locally

*** Development Server
#+BEGIN_SRC bash
# Backend (with auto-reload)
cd backend
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Frontend (with hot reload)
cd ui  
npm start
#+END_SRC

*** Development with Debug
#+BEGIN_SRC bash
# Backend with debug logging
cd backend
PYTHONPATH=. python -m uvicorn main:app --reload --log-level debug

# Frontend with verbose output
cd ui
npm start --verbose
#+END_SRC

** Testing

*** Backend Tests
#+BEGIN_SRC bash
# Run all tests
cd backend
python -m pytest

# Run with coverage
python -m pytest --cov=.

# Run specific test file
python -m pytest test_auth.py -v

# Test specific function
python -m pytest test_jde_structure.py::test_jde_connection -v
#+END_SRC

*** Frontend Tests
#+BEGIN_SRC bash
# Run all tests
cd ui
npm test

# Run tests with coverage
npm test -- --coverage

# Run tests in watch mode
npm test -- --watch

# Run specific test file
npm test -- src/components/Login.test.js
#+END_SRC

*** Integration Tests
#+BEGIN_SRC bash
# Test API endpoints
cd backend
python test_api_endpoints.py

# Test data flow
python test_data_flow.py

# Test internal bakery ops
curl http://localhost:8000/dev/test-internal-bakery-ops
#+END_SRC

** Development Workflow

*** Adding New Features
1. Create feature branch from ~main~
2. Implement backend changes in ~backend/~
3. Add corresponding frontend components in ~ui/src/~
4. Update API documentation
5. Add tests for new functionality
6. Update schema if needed
7. Test integration points
8. Create pull request

*** Code Standards
- *Backend*: Follow PEP 8 for Python code
- *Frontend*: Use ESLint and Prettier for JavaScript
- *Documentation*: Update README.org for major changes
- *Testing*: Maintain >80% code coverage
- *Logging*: Add appropriate logging for new features

** Contributing Guidelines
1. Follow existing code patterns
2. Add comprehensive logging
3. Include error handling
4. Store data flows in S3
5. Update schema versions as needed
6. Add tests for new functionality
7. Document API changes
8. Update deployment scripts if needed

* Security Considerations

** API Security
- JWT-based authentication with configurable expiration
- LDAP integration support for enterprise authentication
- Role-based access control (RBAC)
- API rate limiting to prevent abuse
- Input validation and sanitization
- CORS configuration for frontend access

** Data Security
- Encrypted data in transit (HTTPS/TLS)
- S3 server-side encryption for data at rest
- Database connection encryption
- Secure credential management using environment variables
- No sensitive data in logs
- Password hashing for local authentication

** Network Security
- Internal API endpoints isolated from external access
- Database connections through encrypted channels
- VPC configuration for AWS resources
- Firewall rules for production deployment
- Regular security updates

** Compliance
- Audit trail in S3 with immutable logs
- Data retention policies implementation
- Schema version tracking for data governance
- Access logging for compliance reporting
- GDPR compliance considerations (if applicable)

* Deployment Strategies

** Development Deployment
- Local development with hot reload
- SQLite database for quick setup
- Mock S3 service for testing
- Sample data generation

** Staging Deployment  
- Production-like environment
- Full PostgreSQL database
- Real S3 integration
- Load testing capabilities

** Production Deployment
- High availability setup
- Database clustering
- Load balancing
- Monitoring and alerting
- Backup and recovery procedures

** Scaling Considerations
- Horizontal scaling with multiple backend instances
- Database read replicas
- S3 for distributed storage
- CDN for frontend assets
- Microservices architecture for large deployments

* Support & Maintenance

** Documentation
- *API Documentation*: Available at ~/docs~ endpoint
- *Schema Documentation*: Auto-generated from database
- *Architecture Diagrams*: In ~/docs~ folder
- *Deployment Guides*: In ~/deploy~ directory

** Monitoring Tools
- Health check endpoints
- Metrics collection
- Log aggregation
- Performance monitoring
- Alerting system

** Backup & Recovery
- Database backups (automated)
- S3 data lake redundancy
- Configuration backups
- Disaster recovery procedures

** Contact Information
- *System Administrator*: [Insert contact details]
- *Development Team*: [Insert contact details]  
- *Business Users*: [Insert contact details]
- *Emergency Contact*: [Insert 24/7 support details]

* Version History
- *v2.0.0*: Internal Bakery Operations system with S3 data lake
- *v1.x.x*: Original external Bakery-System integration (deprecated)

* License
[Insert license information]

* Appendix

** Useful Commands Reference
#+BEGIN_SRC bash
# System Status
sudo systemctl status stical-data-backend stical-data-frontend

# View Logs  
journalctl -u stical-data-backend -f
tail -f /var/log/stical-data-backend.log

# Database Operations
psql $PG_DATABASE_URL -c "\dt"  # List tables
psql $PG_DATABASE_URL -c "SELECT * FROM schema_versions ORDER BY created_at DESC LIMIT 5;"

# S3 Operations
aws s3 ls s3://bakery-operations-data-lake/jde-ingestion/ --recursive

# API Testing
curl -X POST http://localhost:8000/dev/initialize-sample-data
curl http://localhost:8000/health
curl http://localhost:8000/bakeryops/facilities/default_facility/products
#+END_SRC

** Environment Variables Reference
#+BEGIN_SRC bash
# Complete .env template
PG_DATABASE_URL=postgresql://username:password@localhost:5432/bakery_operations_db
DB_NAME=bakery_operations_db
BACKEND_BASE_URL=http://localhost:8000
FACILITY_ID=default_facility
JDE_BUSINESS_UNIT=1110
JDE_CARDEX_URL=https://your-jde-server/jderest/v3/orchestrator/JDE_CARDEX_SUMMARY
JDE_CARDEX_USERNAME=your_username
JDE_CARDEX_PASSWORD=your_password
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
S3_BUCKET_NAME=bakery-operations-data-lake
SECRET_KEY=your-secret-key-change-this-in-production
ALGORITHM=HS256
#+END_SRC
